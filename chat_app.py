import audioop
import base64
import hashlib
import io
import os
import re
import time
import wave

import requests
import streamlit as st
try:
    import google.generativeai as genai
except ModuleNotFoundError:
    genai = None
try:
    from openai import OpenAI
except ModuleNotFoundError:
    OpenAI = None
try:
    import parsedatetime as pdt
except ModuleNotFoundError:
    pdt = None
try:
    import dateparser
except ModuleNotFoundError:
    dateparser = None

API = "https://dualsubstrate-commercial.fly.dev"
ENTITY = "demo_user"
def _secret(key: str):
    try:
        return st.secrets.get(key)
    except Exception:
        return None

API_KEY = _secret("DUALSUBSTRATE_API_KEY") or os.getenv("DUALSUBSTRATE_API_KEY") or "demo-key"
HEADERS = {"x-api-key": API_KEY} if API_KEY else {}

GENAI_KEY = _secret("API_KEY") or os.getenv("API_KEY")
if genai and GENAI_KEY:
    genai.configure(api_key=GENAI_KEY)

OPENAI_API_KEY = _secret("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")

st.set_page_config(page_title="Ledger Chat", layout="centered")
st.title("Ledger Chat with Persistent Memory")
st.caption("Speak or type. Everything anchors to the DualSubstrate ledger.")

# Custom CSS for styling the input box and surrounding elements
st.markdown("""
<style>
    /* Note: These class names may be dynamically generated by Streamlit and could break with updates. */

    /* Remove the border from the text input's direct container */
    .st-emotion-cache-1bcyifm {
        border: none;
    }

    /* Add a border to the parent container that holds both the text input and the mic icon */
    .st-emotion-cache-18kf3ut.e1wguzas4 {
        border: 1px solid rgba(49, 51, 63, 0.2);
        border-radius: 0.5rem;
        padding: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

TIME_PATTERN = re.compile(r"\b(\d{1,2}:\d{2}(?::\d{2})?\s?(?:am|pm)?)\b", re.IGNORECASE)
CAL = pdt.Calendar() if pdt else None
KEYWORD_PATTERN = re.compile(r"\b(quote|verbatim|exact|recall|retrieve|what did i say)\b", re.I)
PREFIXES = ("/q", "@ledger", "::memory")
MODE_OPTIONS = ["Chat", "Exact quotes", "Time search"]

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "last_audio_digest" not in st.session_state:
    st.session_state.last_audio_digest = None
if "typed_input" not in st.session_state:
    st.session_state.typed_input = ""
if "clear_typed" not in st.session_state:
    st.session_state.clear_typed = False
if st.session_state.clear_typed:
    st.session_state.typed_input = ""
    st.session_state.clear_typed = False
if "ledger_state" not in st.session_state:
    st.session_state.ledger_state = None
if "recall_payload" not in st.session_state:
    st.session_state.recall_payload = None


def _normalize_audio(raw_bytes: bytes) -> io.BytesIO:
    # The OpenAI API expects a file with a name.
    buf = io.BytesIO(raw_bytes)
    buf.name = "input.wav"
    with wave.open(buf, "rb") as wf:
        params = wf.getparams()
        audio = wf.readframes(params.nframes)
        sampwidth = params.sampwidth
        channels = params.nchannels
        rate = params.framerate
    if sampwidth != 2:
        audio = audioop.lin2lin(audio, sampwidth, 2)
        sampwidth = 2
    if channels != 1:
        audio = audioop.tomono(audio, sampwidth, 0.5, 0.5)
        channels = 1
    target_rate = 16000
    if rate != target_rate:
        audio, _ = audioop.ratecv(audio, sampwidth, channels, rate, target_rate, None)
    peak = audioop.max(audio, sampwidth) or 1
    if peak < 8000:
        audio = audioop.mul(audio, sampwidth, min(4.0, 20000 / peak))
    buf = io.BytesIO()
    with wave.open(buf, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(target_rate)
        wf.writeframes(audio)
    buf.seek(0)
    return buf


PRIME_ARRAY = (2, 3, 5, 7, 11, 13, 17, 19)
PRIME_WORDS = ("identity", "memory", "voice", "ledger", "trust", "security", "insight", "story")
WORD_TO_PRIME = {w: p for w, p in zip(PRIME_WORDS, PRIME_ARRAY)}
FALLBACK_PRIME = PRIME_ARRAY[0]


def _hash_text(text: str):
    tokens = re.findall(r"[A-Za-z]+", text)
    return [{"prime": WORD_TO_PRIME.get(tok.lower(), FALLBACK_PRIME), "delta": 1} for tok in tokens][:30]


def _cosine(a, b):
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = sum(x * x for x in a) ** 0.5
    norm_b = sum(y * y for y in b) ** 0.5
    if not norm_a or not norm_b:
        return 0.0
    return dot / (norm_a * norm_b)


def _semantic_score(prompt: str) -> float:
    if not (OpenAI and OPENAI_API_KEY):
        return 0.0
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        target = "provide exact quotes from prior user statements"
        emb_prompt = client.embeddings.create(model="text-embedding-3-small", input=prompt).data[0].embedding
        emb_target = client.embeddings.create(model="text-embedding-3-small", input=target).data[0].embedding
        return _cosine(emb_prompt, emb_target)
    except Exception:
        return 0.0


def _memory_lookup(limit: int = 3, since: int | None = None):
    params = {"entity": ENTITY, "limit": limit}
    if since:
        params["since"] = since
    try:
        resp = requests.get(f"{API}/memories", params=params, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except requests.RequestException:
        return []


def _render_memories(entries):
    if not entries:
        st.info("No matching memories.")
        return
    for entry in entries:
        stamp = entry.get("timestamp")
        ts = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stamp / 1000)) if stamp else "unknown time"
        text = entry.get("text", "(no text)")
        msg = f"{ts} â€” {text}"
        st.session_state.chat_history.append(("Memory", msg))
        st.info(msg)


def _augment_prompt(user_question: str) -> str:
    try:
        resp = requests.get(
            f"{API}/memories",
            params={"entity": ENTITY, "limit": 5},
            headers=HEADERS,
            timeout=10,
        )
        resp.raise_for_status()
        memories = resp.json()
    except requests.RequestException:
        return user_question

    if not memories:
        return user_question

    quotes = "\n".join(f"- {m.get('text','')}" for m in memories if m.get("text"))
    if not quotes:
        return user_question

    return (
        "User asks for exact quotes.\n"
        "Here are the user's own anchored statements:\n"
        f"{quotes}\n\n"
        "Reply with THREE verbatim extracts (keep punctuation, capitalisation, "
        "and use quotation marks). Do not paraphrase.\n"
        f"User question: {user_question}"
    )


def _maybe_handle_recall_query(text: str) -> bool:
    prefix = text.strip().lower().startswith(PREFIXES)
    keyword = KEYWORD_PATTERN.search(text) is not None
    since_ms = None

    parsed_datetime = None
    parsed_epoch = None
    if dateparser:
        parsed_datetime = dateparser.parse(text, settings={"PREFER_DATES_FROM": "past"})
    if CAL:
        match = TIME_PATTERN.search(text)
        if match:
            parsed_tuple, status = CAL.parse(match.group(1))
            if status != 0:
                parsed_epoch = int(time.mktime(parsed_tuple) * 1000)
                if dateparser:
                    parsed_str = time.strftime("%Y-%m-%d %H:%M:%S", parsed_tuple)
                    parsed_datetime = dateparser.parse(parsed_str)
    if parsed_datetime:
        since_ms = int(parsed_datetime.timestamp() * 1000)
    elif parsed_epoch:
        since_ms = parsed_epoch

    semantic = _semantic_score(text)
    prefix_score = 1.0 if prefix else 0.0
    keyword_score = 1.0 if keyword else 0.0
    time_score = 1.0 if since_ms else 0.0
    scores = [keyword_score, time_score, semantic, prefix_score]
    weights = [0.3, 0.4, 0.2, 0.1]
    weighted_total = sum(s * w for s, w in zip(scores, weights))

    if weighted_total > 0.45:
        entries = _memory_lookup(limit=3, since=since_ms)
        _render_memories(entries)
        return True
    return False


def _anchor(text: str):
    factors = _hash_text(text)
    if not factors:
        st.warning("No alphabetical tokens detected; nothing anchored.")
        return False
    payload = {"entity": ENTITY, "factors": factors, "text": text}
    resp = requests.post(f"{API}/anchor", json=payload, headers=HEADERS, timeout=10)
    try:
        resp.raise_for_status()
    except requests.HTTPError as exc:
        st.error(f"Anchor failed ({resp.status_code}): {resp.text}")
        return False
    st.session_state.chat_history.append(("You", text))
    st.success("Anchored into ledger.")
    return True


def _recall():
    resp = requests.get(f"{API}/retrieve?entity={ENTITY}", headers=HEADERS, timeout=10)
    if resp.ok:
        st.session_state.recall_payload = resp.json()
    else:
        st.session_state.recall_payload = {"error": resp.text}


def _load_ledger():
    resp = requests.get(f"{API}/ledger", params={"entity": ENTITY}, headers=HEADERS, timeout=10)
    st.session_state.ledger_state = resp.json() if resp.ok else {"error": resp.text}


def _chat_response(prompt: str, use_openai=False):
    prompt = _augment_prompt(prompt)
    if use_openai:
        if not (OpenAI and OPENAI_API_KEY):
            st.warning("OpenAI API key missing.")
            return
        client = OpenAI(api_key=OPENAI_API_KEY)
        messages = [{"role": "user", "content": prompt}]
        response = client.chat.completions.create(model="gpt-3.5-turbo", messages=messages)
        full = response.choices[0].message.content
        st.session_state.chat_history.append(("Bot", full))
        return full

    if not (genai and GENAI_KEY):
        return "Gemini API key missing."
    model = genai.GenerativeModel("gemini-2.0-flash")
    chat = model.start_chat(history=[{"role": "user", "parts": [h[1]]} for h in st.session_state.chat_history])
    chunks = []
    for chunk in chat.send_message(prompt, stream=True):
        chunks.append(chunk.text)
    full = "".join(chunks).strip()
    st.session_state.chat_history.append(("Bot", full))
    return full or "(No response)"


st.sidebar.header("Live Memory")
if st.sidebar.button("Recall"):
    _recall()
if st.sidebar.button("Load ledger"):
    _load_ledger()

if st.session_state.recall_payload:
    st.sidebar.write("Last recall:", st.session_state.recall_payload)
if st.session_state.ledger_state:
    st.sidebar.write("Ledger:", st.session_state.ledger_state)

# ---------- investor KPI ----------
st.sidebar.header("Investor KPI")
col1, col2, col3 = st.sidebar.columns(3)

# fetch once, with error handling
try:
    metrics_resp = requests.get(f"{API}/metrics", headers=HEADERS, timeout=5)
    metrics_resp.raise_for_status()
    metrics = metrics_resp.json()
except (requests.RequestException, ValueError):
    metrics = {"tokens_deduped": "N/A", "ledger_integrity": 0.0}

try:
    memories_resp = requests.get(f"{API}/memories", params={"entity": ENTITY, "limit": 1}, headers=HEADERS, timeout=5)
    memories_resp.raise_for_status()
    memories = memories_resp.json()
except (requests.RequestException, ValueError):
    memories = []

# Safely access memories and metrics
oldest = memories[-1].get("timestamp", time.time() * 1000) / 1000 if isinstance(memories, list) and memories else time.time()
durability_h = (time.time() - oldest) / 3600
tokens_saved = metrics.get("tokens_deduped", "N/A")
ledger_integrity = metrics.get("ledger_integrity", 0.0)

col1.metric("ðŸ’° Tokens Saved", tokens_saved)
col2.metric("ðŸ”’ Integrity %", f"{ledger_integrity*100:.1f} %",
delta=None, delta_color="normal")
col3.metric("ðŸ§± Durability h", f"{durability_h:.1f}")

# green/red colour
if ledger_integrity >= 0.995:
    col2.markdown("âœ…")
if durability_h >= 24:
    col3.markdown("âœ…")

col_text, col_voice = st.columns([4, 1])

with col_text:
    with st.form("typed_chat"):
        typed_text = st.text_input("Enter a prompt here", key="typed_input")
        submitted = st.form_submit_button("Send")
    if submitted:
        text = typed_text.strip()
        if not text:
            st.warning("Enter some text first.")
        elif _maybe_handle_recall_query(text):
            st.session_state.clear_typed = True
            st.rerun()
        elif _anchor(text):
            _chat_response(text, use_openai=True)
            st.session_state.clear_typed = True
            st.rerun()

with col_voice:
    audio = st.audio_input("Hold to talk", key="voice_input")
    if audio:
        audio_bytes = audio.getvalue()
        digest = hashlib.sha1(audio_bytes).hexdigest()
        if digest != st.session_state.last_audio_digest:
            st.session_state.last_audio_digest = digest
            norm = _normalize_audio(audio_bytes)
            if not (OpenAI and OPENAI_API_KEY):
                st.warning("OpenAI API key missing.")
            else:
                client = OpenAI(api_key=OPENAI_API_KEY)
                try:
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=norm
                    )
                    text = transcript.text
                    st.write(f"Transcript: {text}")
                    if text and _maybe_handle_recall_query(text):
                        pass
                    elif text and _anchor(text):
                        _chat_response(text, use_openai=True)
                except Exception as exc:
                    st.error(f"Transcription failed: {exc}")

st.subheader("Chat History")
for role, content in st.session_state.chat_history[-20:]:
    st.markdown(f"**{role}:** {content}")
st.divider()
if st.button("Refresh ledger snapshot"):
    _load_ledger()
if st.session_state.ledger_state:
    st.json(st.session_state.ledger_state)
